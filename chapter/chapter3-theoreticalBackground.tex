\chapter{Theoretical Background}

\section{Convolution Neural Network - CNN}

Convolution Neural Networks are a particular class of Neural Networks \cite{masood2018real}. They are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product, and optionally follows it with a non-linearity. CNN mainly consists of Convolution Layers, Pooling Layers, Activation Layers, and Fully Connected Layers. ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode specific properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the number of parameters in the network. Some of the primary uses of CNN can be mentioned as image classification, object detection, semantic segmentation, face recognition, ...

%Insert picture about CNN https://scholarworks.iupui.edu/bitstream/handle/1805/24768/FINAL%20Prasham_Shah_Thesis%20.pdf?sequence=1&isAllowed=y
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/Cover.png}
	\caption{Convolution Neural Network}
	\label{fig:Chap3-OverviewTheCNN}
\end{figure}

The figure \ref{fig:Chap3-OverviewTheCNN} above shows an example of a convolution neural network, which is taking an image as input and then extracting features from it through various layers and then finally predicting the class of the object in the given image.

\subsection{Architecture}

Convolution Neural Networks have a different architecture than regular Neural Networks, and we can see this difference in figure \ref{fig:Chap3-DiffArchCNN_NNN} below. Regular Neural Networks transform an input through a series of hidden layers. Every layer comprises a set of neurons, where each layer is fully connected to all neurons in the previous layers. Finally, a last fully-connected output layer represents the predictions with CNN architecture. First of all, the layers are organized into three dimensions: width, height, and depth. Further, the neurons in one layer do not connect to all the neurons in the next layer but only to a small region. Lastly, the system will reduce the final output to a single vector of probability scores, organized along the depth dimension.

%FIXME: Insert image of diff architecture : https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/DiffArchCNN-ANN}
	\caption{Different between Normal Neural Network and Convolution Neural Network}
	\label{fig:Chap3-DiffArchCNN_NNN}
\end{figure}

%FIXME: Insert image of CNN arc: https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/CNN-Arch}
	\caption{Convolution Neural Network Architecture}
	\label{fig:Chap3-CNN_Arch}
\end{figure}

As we can see in figure \ref{fig:Chap3-CNN_Arch}, CNN can be divided into two parts:
\begin{enumerate}
	\item The hidden layers/ Feature extraction part\\
	In this part, the network will perform a series of convolutions and pooling operations during which the features are detected. If you had a picture of a zebra, this is the part where the network would recognize its stripes, two ears, and four legs.
	\item The Classification part\\
	The fully connected layers will serve as a classifier on top of their extracted features. They will assign a probability for the object on the image being what the algorithm predicts it is.
\end{enumerate}

\subsection{Feature extraction part}
\subsubsection{Convolutional Layer}

The convolution layer is the core building block of a Convolutional Network that does most of the computational heavy lifting. A convolution is executed by sliding the filter over the input. At every location, matrix multiplication is performed and sums the result onto the feature map. This extracting features from images happen throughout the CNN's convolutional layers. This process is illustrated in figure \ref{fig:Chap3-CNN_Layer}

% FIXME: https://scholarworks.iupui.edu/bitstream/handle/1805/24768/FINAL%20Prasham_Shah_Thesis%20.pdf?sequence=1&isAllowed=y
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/ConvLayer}
	\caption{Convolution Neural Network Layer}
	\label{fig:Chap3-CNN_Layer}
\end{figure}

When the feature map is made, we can pass each value in the feature map through a non-linearity function, such as ReLU, sigmoid before it becomes the input of the next convolution layer.

Due to the size of the feature map being always smaller than the input, we have to do something to prevent our feature map from shrinking. This is where we use padding (\ref{fig:Chap3-CNN_Padding}). A layer of zero-value pixels is added to surround the input with zeros so that our feature map will not shrink. In addition to keeping the spatial size constant after performing convolution, padding also improves performance and ensures the Kernel and stride size will fit in the input.

% FIXME: => Need picture
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/CNN_Padding}
	\caption{Using padding for strike one in Convolution Layer}
	\label{fig:Chap3-CNN_Padding}
\end{figure}
\subsubsection{Pooling Layers}

After a convolution layer, it is common to add a pooling layer in between CNN layers. The function of Pooling is to continuously reduce the dimensionality to reduce the number of parameters and computation in the network. This action shortens the training time and controls overfitting.

There are mainly two types of Pooling Layers in a CNN: Max Pooling and Average Pooling. The functionality of these two types of layers is demonstrated in figure \ref{fig:Chap3-CNN_Pooling}. Max Pooling restores the maximum value from the picture segment covered by the Kernel. Average Pooling converts the average values from the bit of the picture surrounded by the Kernel.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/Pooling}
	\caption{Max Pooling and Average Pooling}
	\label{fig:Chap3-CNN_Pooling}
\end{figure}

% FIXME: Insert image about max pooling and average pooling

\subsubsection{Activation Layers}

In general, Neural networks and CNNs rely on a non-linear "trigger" function to signal distinct identification of likely features on each hidden layer. CNN may use a variety of specific functions (figure \ref{fig:Chap3-CNN_ActiveFunction}), such as rectified linear units (ReLUs) and continuous trigger (non-linear) functions—to efficiently implement this non-linear triggering.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/ActiveFunction}
	\caption{ Some Active Function common used in CNN }
	\label{fig:Chap3-CNN_ActiveFunction}
\end{figure}
% FIXME: Insert picture of some function like ReLU, tanh ....
\subsection{Classification part}
\subsubsection{Fully connected layers}

The last layers of a CNN are fully connected. Neurons in a fully connected layer have complete connections to all the activations in the previous layer. This part is, in principle, the same as a regular Neural Network.

Figure \ref{fig:Chap3-FC} illustrates the way of input value stream into the fully connected layer. Because these fully connected layers can only accept one-dimensional data, we need to convert our 3D data to 1D data. After passing through some FC, we will get the data classification result.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/FC}
	\caption{ Fully connected Layer}
	\label{fig:Chap3-FC}
\end{figure}
% FIXME: Insert picture ...
\section{Media Pipe}\label{sec:MediaPipe}
% FIXME: Lấy từ eureka bỏ vào
\subsection{Introduction to Media Pipe Hands}
MediaPipe Hands (\ref{fig:Chap3-MediaPipe}) is a high-resolution tracking system for hands and fingers \cite{zhang2020mediapipe}. It uses machine learning to infer 21 3D hand landmarks from a single frame. This solution delivers real-time performance on a cell phone and even scales to many hands, whereas current state-of-the-art systems rely primarily on powerful desktop environments for inference.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/Chap3/Media Pipe}
	\caption{ Media Pipe real time tracking 3D hand landmarks}
	\label{fig:Chap3-MediaPipe}
\end{figure}

MediaPipe Hands makes use of a machine learning pipeline that consists of several models that work together: A palm detection model, which acts on the entire image, will return an orientated hand bounding box. A hand landmark model that returns high-fidelity 3D hand key points from the cropped image region determined by the palm detector.

However, providing the hand landmark model with a correctly cropped hand image minimizes the requirement for data augmentation drastically (such as rotations, translations, and scaling) and instead allows the network to focus on coordinate prediction accuracy. Furthermore, in this ML pipeline, crops can be created based on the hand landmarks recognized in the previous frame, and palm detection is only used to localize the hand when the landmark model can no longer detect its presence.
\subsection{Palm detection model}
The Media Pipe team provides the palm detection model to detect initial hand locations and distinguish whether the hand recognized is left or right, which is very useful as each sign goes along with a different side will result in different meanings. They created a single-shot detector model, comparable to the face detection model in MediaPipe Face Mesh \cite{MediaPipeFaceMesh}, tailored for mobile real-time applications. Hand detection is difficult: our model must detect occluded and self-occluded hands and work across many hand sizes with a significant scale span relative to the image frame.

According to their statement, the methods they use to address the above challenges vary in many strategies. First, instead of training a hand detector, they train a palm detector because estimating bounding boxes of inflexible objects like palms and fists is much easier than recognizing hands with articulated fingers. Furthermore, the non-maximum suppression method performs effectively even in two-hand self-occlusion situations such as handshakes because palms are small objects. Furthermore, palms can be simulated using square bounding boxes (anchors in ML language) that ignore other aspect ratios, reducing 3-5 anchors. Second, even for tiny objects, an encoder-decoder feature extractor is used for more extensive picture context awareness (similar to the Retina Net approach). Finally, the significant scale variance limits focus loss during training to support many anchors.

Using the strategies described above gives an average precision of 95.7 percent in palm detection. With no decoder and a regular cross-entropy loss, the baseline is just 86.22 percent.

\subsection{Hand landmark model}
Following palm detection over the entire image, our next hand landmark model uses regression to accomplish exact key point localization of 21 3D hand-knuckle coordinates (see figure \ref{fig:Chap3-HandLandMark}) within the detected hand regions, i.e., direct, coordinate prediction. Even with partially visible hands and self-occlusions, the model develops a consistent internal hand posture representation.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/HandLandMark}
	\caption{ 21 Hand Landmarks }
	\label{fig:Chap3-HandLandMark}
\end{figure}


\section{Distance Matrix}

A distance matrix \cite{DistanceMatrix} is a table that shows the distance between pairs of objects. For example, in the figure \ref{fig:Chap3-DM}., we can see the length between A and B is 16, B and C is 37, and so on. In the diagonal of the table is the distance to the object from itself, so the value, as we can see, is 0. Distance matrices are sometimes called dissimilarity matrices.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{img/Chap3/DM}
	\caption{ Distance Matrix }
	\label{fig:Chap3-DM}
\end{figure}

% FIXME: Insert picture of distance matrix

\subsection{Create Distance Matrix}

A distance matrix is computed from a raw data table. In the example below (figure \ref{fig:Chap3-DM_Formula}), we can use high school math (Pythagoras) to work out the distance between A and B.

% FIXME: Chèn công thức vào đây
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/Chap3/DM_Formula}
	\caption{ Calculating distance between A and B}
	\label{fig:Chap3-DM_Formula}
\end{figure}

We can use the same formula with more than two variables, known as the Euclidean distance. As a result, we have the distance matrix represented like figure \ref{fig:Chap3-DM-Raw}.

% FIXME: chèn bảng kết quả vào
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/Chap3/DM-Raw}
	\caption{ The Distance Matrix is constructed from Raw Data }
	\label{fig:Chap3-DM-Raw}
\end{figure}

\section{Beam search and Connectionist Temporal Classification}
% CheckList:
%   [x] BeamSearch
%   [x] CTC recap
%   [x] Combination
%   [x] Pseudo code
\subsection{Connectionist Temporal Classification}

Connectionist Temporal Classification (CTC) \cite{hannun2017sequence} is a type of Neural Network output helpful in tackling sequence problems like handwriting (figure \ref{fig:Chap3-Overview-CTC}) and speech recognition where the timing varies. Using CTC ensures that one does not need an aligned dataset, which makes the training process
more straightforward.

% FIXME: Insert about CTC in speech recognition
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/Overview-CTC}
	\caption{ Overview of a Neural Network for handwriting recognition }
	\label{fig:Chap3-Overview-CTC}
\end{figure}

\subsection{Why we want to use CTC}

In the context of handwritten recognition, we could create a dataset with images of text-lines, and then specify for each horizontal position of the image the corresponding character as shown in figure \ref{fig:Chap3-Annottion-image-CTC} Then, we could train a model to output a character-score for each horizontal position. However, there are two problems with this solution.

\begin{itemize}
	\item It takes much time, and annotating the dataset at the character level is tiresome.
	\item What if the character takes up more than one time-step ?. We could get "tooo" because the "o" is a wide-character as shown in figure \ref{fig:Chap3-Annottion-image-CTC}. We must remove all duplicate characters like "t" and "o".
\end{itemize}

% FIXME: Insert image ...
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{img/Chap3/Annotation-image-CTC}
	\caption{ Annotation for each horizontal position of the image }
	\label{fig:Chap3-Annottion-image-CTC}
\end{figure}

CTC can solve both problems for us:
\begin{itemize}
	\item We can ignore both the position and width of the character in the image and only requires the text that occurs in the picture.
	\item Using decode techniques, we can directly get the result of the network, and no further post-processing of the recognized text is needed.
\end{itemize}

\subsection{Beam Search with CTC decoder}
CTC has more than the Decoding phase, it can have the Encoding, Loss calculation, but we don't need it in this graduation thesis scope anymore. So, here, we only mention to CTC decoder, but in the way, it combines with Beam Search \cite{scheidl2018word}. Because CTC in decoding context can connect with another algorithm like best-path decoding, ...

\subsubsection{Beam search}

In computer science, beam search \cite{BeamSearch} is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search the reduces its memory requirements. Best-first search is a graph search that orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of the best partial solutions are kept as candidates. Pseudocode for the basic version of beam-search is shown in figure \ref{fig:Chap3-Basic-Version-BeamSearch}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/Chap3/Basic-Version-BeamSearch}
	\caption{ Basic version of Beam Search }
	\label{fig:Chap3-Basic-Version-BeamSearch}
\end{figure}

% FIXME: Insert image about pseudo-code for beam-search

The beam search algorithm will be implemented through the following steps, with two parameters will be included: output matrix and beam width (BW), which specifies the number of beams to keep. First, the beam list and corresponding score are initialized (lines 1 and 2). After that, from 3-15, the algorithm will loop over all time-steps of the matrix output. At this point, only the best scoring beams (equal BW) from the previous time-step are kept (line 4). For each beam, we calculate the score and get a result (line 8); we will cover this step in more detail later. Further, each beam is extended by all possible characters from the alphabet (line 10), and again, a score is calculated (line 11). After the last time-step, the best beams are returned (line 16).

% FIXME: Insert image about beam search

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/Chap3/BeamSearchTree}
	\caption{ NN output and tree of beams with alphabet = {"a", "b"} and BW = 2 }
	\label{fig:Chap3-BSTree}
\end{figure}

As we can see, in figure \ref{fig:Chap3-BSTree}, both output matrix to be decoded, and the tree of beams is shown. Beam search algorithm extended as possible and keep exactly BW candidates. Finally, we finished the last iteration, and the final step of the algorithm is to return the beam with the highest score, which is "a" in this example.

\subsubsection{Calculating the score}

As discussed above, in this part, we will talk about how to score the beam. We will split the beam-score into the score of paths ending with a blank(e.g. 'aa-') and paths ending with non-blank (e.g. 'aaa').

\begin{itemize}
	\item We denote the probability of all paths ending with a blank and corresponding to a beam b at time-step t
	      by $ P_{b}(b,t) $ and by $ P_{nb}(b,t) $ for the non-blank case.
	\item The probability $P_{tot}(b,t)$ of a beam b at time-step t is simply the sum of $P_b$ and $P_{nb}$, for example:
	      $P_{tot}(b,t) = P_b(b,t) + P_{nb}(b,t)$
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/Chap3/CTC_Scoring}
	\caption{ The effect of appending a character to paths ending with blank and non-blank }
	\label{fig:Chap3-CTC_Scoring}
\end{figure}

In figure \ref{fig:Chap3-CTC_Scoring}, we will see what happens when we extend a path. Three main case we can mention
is:
\begin{itemize}
	\item Extend by blank ('a' + '-' = 'a-')
	\item Extend by repeating last character ( 'aa' + 'a' = 'aaa' or 'aa-' + 'a' = 'aa-a')
	\item Extend by some other character ('aa' + 'b' = 'aab')
\end{itemize}

% FIXME: Viết lại các công thức bên dưới
And when we collapse the extended paths, two result we will get and some case we needed to handle:
\begin{itemize}
	\item The unchanged (copied) beam ('a' $ \rightarrow $ 'a'):
	      \begin{itemize}
		      \item To copy a beam, we can extend corresponding paths by a blank and get
		            paths ending with a blank: $ P_b (n, t) += P_{tot}(b, t-1)*mat(blank, t) $
		      \item Beside, with non-blank ending paths case, if we extend it by the last
		            character (the beam is not empty): $ P_{nb}(b,t) += P_{nb}(b,t-1)*mat(b[-1],t) $
		            with -1 indexes the last character in the beam
	      \end{itemize}
	\item An extended beam ('a' $\rightarrow$ 'aa' or 'ab'):
	      \begin{itemize}
		      \item To extend a beam. With the last character is different from the character we need
		            to extend, then there is no need for separating blanks ('-') in the paths:
		            $ P_{nb}(b+c,t) += P_{tot}(b,t-1)*mat(c,t) $
		      \item Or the last character of beam is repeated, we must ensure that the paths
		            end with a blank: $ P_{nb}(b+c,t) += P_b(b,t-1)*mat(c,t) $
		      \item We don't need t care about $P_b(b+c,t)$ because we added a non-blank character
	      \end{itemize}
\end{itemize}

\subsubsection{Putting it all together}

Figure \ref{fig:Chap3-BS_CTC} depicts the CTC beam search algorithm. It is similar to the basic version previously displayed. However, it includes the code to score the beams: copied beams (lines 7-10) and extended beams(lines 15-19). Finally, when looking for the best scoring beams, the programs rank them according to Ptot (line 4) and then take the BW best ones.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{img/Chap3/BS_CTC}
	\caption{ CTC beam search }
	\label{fig:Chap3-BS_CTC}
\end{figure}

\newpage
\section{Technology}
%FIX-ME: Chém gió thêm ra phần này, sử dụng ngôn ngữ gì bla bla 
% Ứng dụng được viết bằng react-native. Sử dụng hệ thống authenticate của firebase
Overall the product application is developed with the technologies including java for android app and system authentication of Firebase.

\subsection{React Native}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/technology/ReactNative.png}
	\caption{React Native Logo}
	\label{fig:ReactNativeLogo}
\end{figure}

Traditional mobile app development necessitates knowledge of two distinct platforms (and programming languages): Android and iOS. However, using React Native\cite{ReactNative}, developers can create hybrid apps that operate on both platforms. The advantages of applying React Native in the app development process contain the following points.

\begin{itemize}
	\item \textbf{Cross-Platform}: One of the significant advantages of React Native is that we can write the same code for both the Android and iOS ecosystems simultaneously, with just minor changes for each platform.
	\item \textbf{One programming language}: There is no need to be familiar with the languages used for platform-specific application development because React Native employs JavaScript, which is currently one of the most popular programming languages\cite{10MostPopularProgrammingLang}. To be more specific, in this project, we use an advanced version of JavaScript, which is known as TypeScript, and we will discuss it in the later section.
	\item \textbf{Performance}: Because both platforms use the same code, React Native allows for the rapid creation of mobile applications. It also has a hot reloading functionality that ensures that modest changes to the program are shown to the developer right away.
\end{itemize}

Therefore, using this React Native framework benefits us in developing the same app for iOS devices from Apple Inc. According to the plan that we came up with before, when the product app works acceptably on Android devices, we will expand it and make it work on those iOS devices without massive effort when translating an app from one operating system to another.

\subsection{TypeScript}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.2\textwidth]{img/technology/JavaScript.png}
	\caption{JavaScript Logo}
	\label{fig:JavaScriptLogo}
\end{figure}

To know about TypeScript, first, we need to know JavaScript. JavaScript is a solid client-side programming language that is open-source. Its primary purpose is to enhance users' interaction with a web page. In other words, developers may make the website more dynamic and exciting by using this programming language. JavaScript is also widely used in the development of games and mobile apps. Excellent speed, cross-browser interoperability, and simple semantics are some of JavaScript's primary qualities, enabling a seamless development experience.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/technology/TypeScript.png}
	\caption{TypeScript Logo}
	\label{fig:TypeScriptLogo}
\end{figure}

On the other hand, TypeScript is a modern JavaScript programming language. It is a statically built language for writing concise JavaScript code. It can be used with Node.js or any browser that supports ECMAScript 3 or above. Static typing, classes, and an interface are all available in TypeScript. Adopting Typescript for a large JavaScript project can result in more robust software easily deployable with a typical JavaScript application.

\begin{itemize}
	\item \textbf{Static typing}: JavaScript is tightly typed, so it does not know what kind of variable it is dealing with until it is practically created at runtime. JavaScript now has type support thanks to TypeScript.
	\item \textbf{Supports new ECMAScript}: TypeScript may trans-pile unique ECMAScript criteria to ECMAScript objectives of our choice. As a result, we can use lambda, modules, functions, the spread operator, de-structuring, and classes, which are all features of ES2015 and beyond.
	\item \textbf{Type inference}: The use of type inference in TypeScript makes typing more manageable and less confusing. Even if we do not use the interface, TypeScript can help us avoid mistakes that could cause runtime issues.
	\item \textbf{Interoperability}: TypeScript is inextricably linked to JavaScript. As a result, it has high interoperability capabilities; however, it requires some additional work to integrate with TypeScript JS libraries.
	\item \textbf{Null examination strict}: In JavaScript software programming, errors, like cannot read property 'x' of undefined, are common. We may avoid most of these mistakes because a variable unknown to the TypeScript compiler cannot be used.
\end{itemize}

\subsection{Native Base}

\subsection{Firebase}
Firebase is a web and mobile app development platform that includes easy and powerful APIs without requiring a backend or server. Firebase is based on the cloud platform. Google's server system is also present. Its principal purpose is to make it easier for users to program apps by simplifying database operations. Users can store and synchronize data using the real-time database service. This service is entirely cloud-based. They will consume up the device's memory if it is offline and then automatically sync to the server once it is online.

\subsection{Java}
JAVA was developed by James Gosling at Sun Microsystems Inc in the year 1995, later acquired by Oracle Corporation. It is a simple programming language. Java makes writing, compiling, and debugging programming easy. It helps to create reusable code and modular programs.

Java is a class-based, object-oriented programming language and is designed to have as few implementation dependencies as possible. A general-purpose programming language made for developers to write once run anywhere that is compiled Java code can run on all platforms that support Java. Java applications are compiled to byte code that can run on any Java Virtual Machine. The syntax of Java is similar to c/c++.Overall the product application is developed with the technologies including react-native for front-end and system authentication of Firebase.
