\chapter{Theoretical Background}

Lorem ipsum

CheckList:
    [x] Mô hình Convolution Neural Network - CNN
    [...] Media Pipe -> Lấy từ eureka
    [x] Distance Matrix
    [x] BeamSearch with CTC decode

  \section{ Convolution Neural Network - CNN }
    Convolution Neural Networks are a special class of Neural Networks. They are made up 
    of neurons that have learnable weights and biases. Each neuron receives some inputs, 
    performs a dot product and optionally follows it with a non-linearity. CNN mainly consist
    of Convolution Layers, Pooling Layers, Activation Layers and Fully Connected Layers.
    ConvNet architectures make the explicit assumption that the inputs are images, 
    which allows us to encode certain properties into the architecture. These then 
    make the forward function more efficient to implement and vastly reduce the amount 
    of parameters in the network. Some of the main uses of CNN can be mentioned as: image
    classification, object detection, semantic segmentation, face recognition, etc.
    
    \\Insert picture about CNN https://scholarworks.iupui.edu/bitstream/handle/1805/24768/FINAL%20Prasham_Shah_Thesis%20.pdf?sequence=1&isAllowed=y
    
      The figure above shows an example of convolution neural network, which is taking an image 
    as input and then extracting features from it through various layers and then finally predicting the
    class of the object in the given image

    \subsection{ Architecture }
      Convolutional Neural Networks have a different architecture than regular Neural Networks.
      Regular Neural Networks transform an input by putting it through a series of hidden layers. 
      Every layer is made up of a set of neurons, where each layer is fully connected to all neurons 
      in the layer before. Finally, there is a last fully-connected layer — the output layer — that 
      represent the predictions.
      Convolutional Neural Networks are a bit different. First of all, the layers are organised in 
      3 dimensions: width, height and depth. Further, the neurons in one layer do not connect to all 
      the neurons in the next layer but only to a small region of it. Lastly, the final output will 
      be reduced to a single vector of probability scores, organized along the depth dimension.
        # Insert image of diff architecture : https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/
      CNN can be divided into two parts:
        + The hidden layers/ Feature extraction part
          In this part, the network will perform a series of convolutions and 
          pooling operations during which the features are detected. If you had a 
          picture of a zebra, this is the part where the network would recognise 
          its stripes, two ears, and four legs.
        + The Classification part
          Here, the fully connected layers will serve as a classifier on top of 
          these extracted features. They will assign a probability for the object 
          on the image being what the algorithm predicts it is.
        # Insert image of CNN arc: https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/
    \subsection{ Feature extraction part }
      \subsubsection{ Convolutional Layer }
        Convolution Layer is the core building block of a Convolutional Network that does most
        the computational heavy lifting. A convolution is executed by sliding the filter over
        the input. At every location, a matrix multiplication is performed and sums the result onto 
        the feature map. This process of extracting features from image happens throughout the CNN's convolutional
        layers. This process is illustrated in Fig...

        # https://scholarworks.iupui.edu/bitstream/handle/1805/24768/FINAL%20Prasham_Shah_Thesis%20.pdf?sequence=1&isAllowed=y

        When the feature map is made, we can pass each value in
        the feature map through a non-linearity function, such as ReLU, sigmoid, etc. Before it
        becomes the input of the next convolution layer.

        Because the size of the feature map is always smaller than the input, we have 
        to do something to prevent our feature map from shrinking. This is where we use 
        padding. A layer of zero-value pixels is added to surround the input with zeros, 
        so that our feature map will not shrink. In addition to keeping the spatial 
        size constant after performing convolution, padding also improves performance 
        and makes sure the kernel and stride size will fit in the input. => Need picture
      \subsubsection{ Pooling Layers }
        After a convolution layer, it is common to add a pooling layer in between CNN layers. 
        The function of pooling is to continuously reduce the dimensionality to reduce the number 
        of parameters and computation in the network. This shortens the training time and controls 
        overfitting.

        There are mainly two types of Pooling Layers in a CNN: Max Pooling and Average
        Pooling. The functionality of these two types of layers are demonstrated in Figure
        ... .Max Pooling restores the maximum value from the segment of the picture covered
        by the Kernel. Whereas, Average Pooling restores the average of the multitude of
        values from the bit of the picture covered by the Kernel. => Insert Picture

      \subsubsection{ Activation Layers }
        Neural networks in general and CNNs in particular rely on a non-linear “trigger” function 
        to signal distinct identification of likely features on each hidden layer. CNNs may use a 
        variety of specific functions, such as rectified linear
        units (ReLUs) and continuous trigger (non-linear) functions—to efficiently implement this non-linear triggering.
        # Insert picture of some function like ReLU, tanh ....
    \subsection{ Classification part }
      \subsubsection{ Fully connected layers }
        The last layers of a CNN are fully connected layers. Neurons in a 
        fully connected layer have full connections to all the activations in the previous 
        layer. This part is in principle the same as a regular Neural Network.

        Figure ... illustrate the way of input value stream into the fully connected layer.
        Because these fully connected layer can only accept 1 Dimensional data. So, we need convert our 3D
        data to 1D data. After pass through some FCL, we will get the result is the data
        classification.

        #Insert picture ...
  \section{ Media Pipe }
    # Lấy từ eureka bỏ vào


  \section{ Distance Matrix }
    A distance matrix is a table that shows the distance between pairs of objects.
    For example, in the figure ..., we can see the distance of A and B is 16, B and C is 37
    and so on. In the diagonal of table is the distance of object from itself, so the value
    as we can see is 0. Distance matrices are sometimes called dissimilarity matrices.

    # Insert picture of distance matrix

    \subsection{ Create Distance Matrix }
      A distance matrix is computed from a raw data table (Figure ...). 
      
      In the example below, we can use high school math (Pythagoras) to work out 
      that distance between A and B. 
        # Chèn công thức vào đây
      
      We can use same formula with more than two variables, and this is known as 
      the Euclidean distance.

      In result, we have the distance matrix represented like Figure ...
      # Chèn bảng kết quả vào
  
  \section{ Beam search and Connectionist Temporal Classification }
    CheckList:
      [x] BeamSearch
      [x] CTC recap
      [x] Combination
      [x] Pseudo code
      \subsection{ Connectionist Temporal Classification }
      Connectionist Temporal Classification (CTC) is a type of Neural Network output helpful
      in tackling sequence problems like handwriting and speech recognition where the timing varies.
      Using CTC ensures that one does not need an aligned dataset, which makes the training process
      more straightforward.
      \subsection{ Why we want to use CTC }
        In context of hand written recognition, we could create a data-set with
        images of text-lines, and then specify for each horizontal position of the image
        the corresponding character as shown in Fig ... Then, we could train a model to output
        a character-score for each horizontal position. However, there are two problem with this
        solution.
          + It takes a lot of time, annotating dataset at the character level is a boring task.
          + What if the character takes up more than one time-step ?. We could get "tooo" because
          the "o" is a wide character as shown in Fig..... We have to remove all duplicate character
          like "t" and "o".
          # Insert image ...
        CTC can solves both problem for us:
          + We can ignore both the position and width of the character in the image
          and only requires the text that occurs in the image.
          + Using decode techniques, we can directly get the result of the network and
          no further post-processing of the recongnized text is needed. 

    \subsection{ Beam Search with CTC decoder }
      CTC has more than Decoding phase, it can have the Encoding, Loss calculation, but in
      this graduation thesis scope, we don't need it anymore. So, in here, we only
      mention to CTC decoder but in the way of it combines with Beam Search. Because CTC
      in decoding context, it can combine with another algorithm like best-path decoding, etc...

      \subsubsection{ Beam search }
      In computer science, beam search is a heuristic search algorithm that
      explores a graph by expanding the most promising node in a limited set.
      Beam search is an optimization of best-first search the reduces its memory
      requirements. Best-first search is a graph search which orders all
      partial solutions (states) according to some heuristic. But in beam search,
      only a predetermined number of best partial solutions are kept as candidates.
      Pseudo-code for basic version of beam-search is shown is Fig....

      # Insert image about pseudo-code for beam-search
      
      Beam search algorithm will be implemented through the following steps, with two
      parameter will be included: output matrix and beam width (BW) which specifies the number
      of beams to keep . First of all, the list of beam and corresponding score is 
      initialized (line 1 and 2). After that, from 3-15, the algorithm will loop over all time-steps
      of the matrix output. At this point, only the best scoring beams (equal BW) from the previous
      time-step are kept (line 4). Each of beam, we calculate the score and get result (line 8), we will cover
      this step in more detail later. Further, each beam is extended by all possible characters from
      the alphabet (line 10) and again, a score is calculated (line 11). After the last time-step,
      the best beams is returned as a result (line 16).

      # Insert image about beam search

      As we can see, in Fig ..., both output matrix to be decoded and the tree of beams are shown. 
      Beam search algorithm extended as possible and keep exactly BW candidates . Finally,
      we finished the last iteration and the final step of the algorithm is to return the beam 
      with the highest score, which is “a” in this example.
      
      \subsubsection{ Calculating the score }
      As we just discuss above, in this part, we will talk about how to scoring the beam.
      We will split the beam-score into the score of paths ending with a blank(e.g.. 'aa-')
      and paths ending with non-blank (e.g. 'aaa'). 
      
      + We denote the probability of all paths ending with a blank and corresponding to a beam b at time-step t 
      by Pb(b,t) and by Pnb(b,t) for the non-blank case.
      + The probability Ptot(b,t) of a beam b at time-step t is simply the sum of Pb and Pnb, for example:
      Ptot(b,t) = Pb(b,t) + Pnb(b,t)

      In Fig .., we will see what happens when we extend a path. Three main case we can mention
      is: 
        + Extend by blank ('a' + '-' = 'a-' )
        + Extend by repeating last character ( 'aa' + 'a' = 'aaa' or 'aa-' + 'a' = 'aa-a')
        + Extend by some other character ('aa' + 'b' = 'aab')
      
      And when we collapse the extended paths, two result we will get and some case we needed to handle:
        + The unchanged (copied) beam ('a' -> 'a'):
          + To copy a beam, we can extended corresponding paths by a blank and get
          paths ending with a blank: Pb (n,t) += Ptot(b, t-1)*mat(blank, t)
          + Beside, with non-blank ending paths case, if we extend it by the last
          character (the beam is not empty): Pnb(b,t) += Pnb(b,t-1)*mat(b[-1],t)
          with -1 indexes the last character in the beam
        + An extended beam ('a' -> 'aa' or 'ab'):
          + To extend a beam. With the last character is different from the character we need
          to extend, then there is no need for separating blanks ('-') in the paths:
            Pnb(b+c,t) += Ptot(b,t-1)*mat(c,t)
          + Or the last character of beam is repeated, we must ensure that the paths
          end with a blank: Pnb(b+c,t) += Pb(b,t-1)*mat(c,t)
          + We don't need t care about Pb(b+c,t) because we added a non-blank character
      \subsubsection{ Putting it all together }
        Fig ... depicts the CTC beam search algorithm. It is similar to the basic version
        previously displayed. However, it includes the code to score the beams: copied beams
        (lines 7-10) and extended beams(line 15-19). Finally, when we looking for the best scoring
        beams, the programs ranks them according to Ptot (line 4) and then take the BW best ones.
        ```Insert image about combination of this```
      

